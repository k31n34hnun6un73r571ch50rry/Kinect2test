	Für unser Vorgehen zentral sind die folgenden beiden Bereiche:
	\begin{enumerate}
		\item die technische Umsetzung, d.\,h.
		\begin{itemize}
		\item das korrekte Erkennen und Werten von Gesten einer ausgezeichneten getrackten Person
		\item das korrekte Berechnen notwendiger Bewegungsparameter
		\item die Einbindung in die bestehende Applikation
		\end{itemize}
		\item die Interaktion mit dem Benutzer, d.\,h.
		\begin{itemize}
		\item das Entwerfen intuitiver und eingängiger Gesten für die verschiedenen Zwecke
		\item das Auszeichnen einer getrackten Person als \glqq Master\grqq, der das Programm steuert
		\end{itemize}
	\end{enumerate}
	Wir stellen in diesem Abschnitt die zentralen unmittelbaren Beobachtungen vor, die sich aus der Aufgabenstellung und dem Versuchsaufbau ziehen lassen.\par\bigskip
	Ausgehend von der Aufgabenstellung kann man abstrahierend zwischen zwei primitiven Steuerungsmodi unterscheiden:
	\begin{itemize}
	\item einem Modus, in dem die Kamera verschoben und rotiert werden kann \&
	\item einem Modus, in welchem Objektmanipulationen möglich sind.
	\end{itemize}
	Der Benutzer sollte sich zu jedem Zeitpunkt nur in maximal einem dieser Modi aufhalten, d.\,h. gleichzeitige Kamera- und Objektmanipulation wird ausgeschlossen. Diese Vereinfachung treffen wir, da damit weniger komplexe Gesten benötigt werden und eine solche simultane Manipulation keine praktische Relevanz besitzt. Für Manipulationen, die man sowohl für die Kamera, als auch für Objekte haben will, bietet dies zudem eine geeignete Kapselung, da z.\,B. Rotationsparameter berechnet werden und dann nur entschieden werden muss, ob sie auf die Kamera oder ein Objekt angewendet werden, je nach Modus. Dies reduziert die Gesamtzahl nötiger Gesten.\par\medskip
	Die Kinect ermöglicht ein Tracking des gesamten Körpers für mehrere (genauer sechs) Personen. Wir beschränken uns aus naheliegenden Gründen jedoch auf einen Teil dieses Spektrums:
	\begin{itemize}
		\item Wir benötigen nur eine Person, die die Anwendung (möglichst ungestört) steuert. Eine genauere Auswertung der restlichen Personen, ihrer Skelette etc. ist unnötig.
		\item Die in unserem Anwendungsfall intuitiven Gesten werden ausschließlich mit den Händen (bzw. Armen) durchgeführt.
	\end{itemize}
	Primitive Erkennungsmöglichkeiten eines Masters kann man etwa aus der Entfernung der getrackten Personen zur Kamera und der Position der Personen im Raum gewinnen. Genauere Erklärungen folgen weiter unten.\par 
	Intuitive Gesten für Verschiebungen imitieren das Verschieben eines großen Gegenstands, etwa einer imaginären Box, sodass hier etwa ein Verschieben der flachen Hand in der Luft naheliegt. Für eine intuitive Drehgeste eignet sich die Vorstellung eines imaginären Lenkrads, genauer gesagt einer Lenkkugel, bei der die Rotation um eine Raumachse nach dem Lenkradprinzip erfolgt. Eine intuitive Geste zur Objektauswahl ist offenbar eine Greifgeste.
	\subsection{Priorisierung der Aufgaben}
	%TODO
	\subsection{Aufgabenverteilung im Team}
	Das Projekt wurde zu viert begonnen, wobei einer der Teilnehmer gleich zu Beginn wieder absprang. Die Aufgabenverteilung im Team wurde ab der Einarbeitungsphase mit dem Kinect-System dynamisch vorgenommen. So kristallisierten sich im Laufe der Zeit diverse Zuständigkeitsbereiche heraus, die grob so umrissen werden können:
	\begin{description}
		\item[Mario Janke] kümmerte sich vor allem um die Mathematik im Hintergrund, präziser um die diversen Berechnungen von Parametern aus den vorliegenden Kinect-Daten. Als klar wurde, dass das Projekt aufgrund der Eigenschaften der Kinect zusätzliche Robustheitsmechanismen benötigte, wurde das Management unserer Puffer zusätzlicher Aufgabenbereich.
		\item[Peter Lindner] sorgte vorderrangig für die Strukturierung des Codes. Dies erstreckt sich auf die Umsetzung des objektorientierten Programmierparadigmas mit der Ausarbeitung und Erstellung der Klassenhierarchie. Die im Zuge dessen entstandene Zustandsmaschine wurde in der Folge von ihm verwaltet. Da für deren Existenz zu großen Teilen das beabsichtigte Wirkungsprinzip der Gesten war, schloss sich dem Aufgabenbereich das Gestenmanagement an.
		\item[Patrick Stäblein] war für das Ansammeln grundlegender Informationen zur Kinect -- gerade in der Anfangsphase -- verantwortlich und programmierte einen Großteil der Mastererkennungsmechanismen in der zweiten Phase des Projekts.
	\end{description}
	Parallel zur Arbeit am Programmcode entstand das vorliegende Dokument und später die Vorbereitung der Abschlusspräsentation. Dabei orientierten sich die vom jeweiligen Projektmitglied bearbeiteten Themengebieten an ihren Zuständigkeiten und damit Wissenschwerpunkten aus der Programmierung.
	\subsection{Werkzeuge}
	In diesem Abschnitt sollen die Tools genannt und erklärt werden, die für die Entwicklung unserer Software vordergründig waren.\par
	Das im Zentrum stehende Trackingsystem \emph{Microsoft Kinect Version 2} war durch das Fachgebiet gegeben. Zur Arbeit damit stand ein Raum bereit, der über die Kinect und einen 3D-Kamera-Aufbau zur Projektion verfügte. Ferner durfte die Kinect für Heimtests auch ausgeliehen werden.\par 
	Mit dem Kinect SDK wird eine Softwarelösung namens \emph{Kinect Studio} ausgeliefert, die sich bei der Kinect-Programmierung zum visuellen Debugging eignet. Im Kinect Studio können die verschiedenen Sensoraufnahmen der Kinect nebst der interpretierten Skelette und Hand-States sowie der festgestellten Tiefensituation betrachtet werden. Diese Features können getoggelt oder umgeschaltet werden. Die Tiefenkarte wird per Falschfarbendarstellung und, sofern erwünscht, sogar dreidimensional präsentiert. Das Kinect Studio erwies sich als sehr hilfreich, um die Güte der Kinect-Daten zu überprüfen und zu erkennen, ob die Kinect einen Teil des Aufnahmebereiches fehlinterpretiert. Dies war zum Teil notwendig, um bei der Programmierung schnell und einfach zwischen Fehlern des Programmes und fehlerhaften Kinect-Daten unterscheiden zu können.\par 
	Ebenfalls zum Kinect-SDK gehörig ist der sogenannte \emph{Visual Gesture Builder}, mit dem Gesten(folgen) aufgenommen und in Programme eingespeist werden können. Da wir uns (s.\,u.) für einen anderen Weg der Gestenimplementierung entschieden haben, fanden hiermit nur kleinere Tests in der Anfangsphase statt.\par 
	Die Kinect-API kann mit JavaScript, C++ oder C\# verwendet werden. Da das uns im Rahmen der Aufgabenstellung übermittelte Programm, für das unsere Gestensteuerung gedacht ist, in C++ geschrieben war, verwendeten wir aus Kompatibilitäts- und Einheitlichkeitsgründen heraus ebenfalls C++ und entwickelten und debuggten mit dem \emph{Microsoft Visual Studio 2015} unter \emph{Microsoft Windows 10}.\par 
	Zur Versionsverwaltung nutzten wir das Kommandozeilentool git mit \href{http://github.com}{GitHub}.\par 